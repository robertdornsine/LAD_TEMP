import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
import numpy as np
import os
import time
from pathlib import Path
import matplotlib.pyplot as plt
from typing import Dict, Optional, Tuple
from tqdm import tqdm
import json


# ==========================================
# âœ… æ”¹è¿›1: MixUpæ•°æ®å¢å¼º
# ==========================================
class MixUpAugmentation:
    """
    MixUpæ•°æ®å¢å¼º

    åŸç†ï¼š
    æ··åˆä¸¤ä¸ªæ ·æœ¬ï¼š
        x_mixed = Î» * x1 + (1-Î») * x2
        y_mixed = Î» * y1 + (1-Î») * y2
    å…¶ä¸­ Î» ~ Beta(Î±, Î±)

    ä¼˜åŠ¿ï¼š
    - æ­£åˆ™åŒ–æ•ˆæœï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
    - å¢åŠ æ ·æœ¬å¤šæ ·æ€§
    - å¹³æ»‘å†³ç­–è¾¹ç•Œ

    é€‚ç”¨åœºæ™¯ï¼š
    - å°æ•°æ®é›†ï¼ˆå¦‚Abileneï¼‰
    - å®¹æ˜“è¿‡æ‹Ÿåˆçš„æ¨¡å‹

    å‚è€ƒï¼š
    Zhang et al. "mixup: Beyond Empirical Risk Minimization" ICLR 2018
    """
    def __init__(self, alpha=0.2, prob=0.5):
        """
        Args:
            alpha: Betaåˆ†å¸ƒå‚æ•°ï¼Œè¶Šå¤§æ··åˆè¶Šå‡åŒ€
                   Î±=0.2: è½»åº¦æ··åˆï¼ˆæ¨èï¼‰
                   Î±=1.0: å‡åŒ€æ··åˆ
            prob: åº”ç”¨MixUpçš„æ¦‚ç‡
        """
        self.alpha = alpha
        self.prob = prob

    def __call__(self, images, targets):
        """
        Args:
            images: [B, T, C, H, W] - è¾“å…¥å†å²å¸§
            targets: [B, 1, H, W] - ç›®æ ‡å¸§

        Returns:
            mixed_images, mixed_targets, lambda_value
        """
        if not self.training or np.random.rand() > self.prob:
            return images, targets, 1.0

        batch_size = images.size(0)

        # é‡‡æ ·lambda
        if self.alpha > 0:
            lam = np.random.beta(self.alpha, self.alpha)
        else:
            lam = 1.0

        # éšæœºé…å¯¹
        index = torch.randperm(batch_size, device=images.device)

        # æ··åˆ
        mixed_images = lam * images + (1 - lam) * images[index]
        mixed_targets = lam * targets + (1 - lam) * targets[index]

        return mixed_images, mixed_targets, lam

    def train(self):
        self.training = True

    def eval(self):
        self.training = False


# ==========================================
# âœ… æ”¹è¿›2: æ„ŸçŸ¥æŸå¤±
# ==========================================
class PerceptualLoss(nn.Module):
    """
    æ„ŸçŸ¥æŸå¤± - ä½¿ç”¨é¢„è®­ç»ƒCNNçš„ä¸­é—´ç‰¹å¾

    åŸç†ï¼š
    ä¸ç›´æ¥æ¯”è¾ƒåƒç´ ï¼Œè€Œæ˜¯æ¯”è¾ƒç‰¹å¾ç©ºé—´çš„è·ç¦»ï¼š
        L_perceptual = ||Ï†(pred) - Ï†(target)||Â²
    å…¶ä¸­ Ï† æ˜¯é¢„è®­ç»ƒCNNçš„ç‰¹å¾æå–å™¨

    ä¼˜åŠ¿ï¼š
    - æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§
    - å¯¹å°çš„åƒç´ åç§»ä¸æ•æ„Ÿ
    - ç”Ÿæˆæ›´è‡ªç„¶çš„å›¾åƒ

    å®ç°ï¼š
    ä½¿ç”¨è®­ç»ƒå¥½çš„VisionEncoderæå–ç‰¹å¾
    """
    def __init__(self, feature_extractor, layers=[2, 3], weights=[0.5, 0.5]):
        """
        Args:
            feature_extractor: é¢„è®­ç»ƒçš„CNNï¼ˆå¦‚VisionEncoderçš„modeléƒ¨åˆ†ï¼‰
            layers: ä½¿ç”¨å“ªäº›å±‚çš„ç‰¹å¾ï¼ˆåˆ—è¡¨ç´¢å¼•ï¼‰
            weights: å„å±‚ç‰¹å¾çš„æƒé‡
        """
        super().__init__()
        self.feature_extractor = feature_extractor
        self.layers = layers
        self.weights = weights

        # å†»ç»“ç‰¹å¾æå–å™¨
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

        self.feature_extractor.eval()

    def extract_features(self, x):
        """
        æå–ä¸­é—´å±‚ç‰¹å¾

        Args:
            x: [B, C, H, W]
        Returns:
            list of features
        """
        features = []

        # å‡è®¾feature_extractoræ˜¯ImprovedCNNEncoder
        # é€å±‚æå–
        h = x

        # Stem
        if hasattr(self.feature_extractor, 'stem'):
            h = self.feature_extractor.stem(h)

        # Blocks
        if hasattr(self.feature_extractor, 'blocks'):
            for i, block in enumerate(self.feature_extractor.blocks):
                h = block(h)
                if i in self.layers:
                    features.append(h)

        return features

    def forward(self, pred, target):
        """
        Args:
            pred: [B, 1, H, W] - é¢„æµ‹å›¾åƒ
            target: [B, 1, H, W] - ç›®æ ‡å›¾åƒ
        Returns:
            loss: æ ‡é‡
        """
        # ç¡®ä¿åœ¨è¯„ä¼°æ¨¡å¼
        self.feature_extractor.eval()

        with torch.no_grad():
            target_features = self.extract_features(target)

        pred_features = self.extract_features(pred)

        # è®¡ç®—å„å±‚æŸå¤±
        loss = 0.0
        for i, (pred_feat, target_feat, weight) in enumerate(
            zip(pred_features, target_features, self.weights)
        ):
            loss += weight * F.mse_loss(pred_feat, target_feat)

        return loss


# ==========================================
# âœ… æ”¹è¿›3: å¸¦é‡å¯çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡
# ==========================================
class CosineAnnealingWarmRestarts:
    """
    å¸¦é‡å¯çš„ä½™å¼¦é€€ç«

    åŸç†ï¼š
    å‘¨æœŸæ€§åœ°é‡å¯å­¦ä¹ ç‡ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜

    å­¦ä¹ ç‡æ›²çº¿ï¼š
        |
        |  â•±â•²      â•±â•²      â•±â•²
        | â•±  â•²    â•±  â•²    â•±  â•²
        |â•±    â•²  â•±    â•²  â•±    â•²
        +--------------------â†’ epoch
             T0    2T0    3T0

    å‚æ•°ï¼š
        T_0: ç¬¬ä¸€æ¬¡é‡å¯çš„å‘¨æœŸ
        T_mult: å‘¨æœŸå€å¢å› å­ï¼ˆé€šå¸¸ä¸º1æˆ–2ï¼‰
        eta_min: æœ€å°å­¦ä¹ ç‡

    ä¼˜åŠ¿ï¼š
    - é€ƒç¦»å±€éƒ¨æœ€ä¼˜
    - å¤šæ¬¡æ”¶æ•›æœºä¼š
    - é€‚åˆé•¿è®­ç»ƒ
    """
    def __init__(self, optimizer, T_0, T_mult=1, eta_min=1e-7, last_epoch=-1):
        self.optimizer = optimizer
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.last_epoch = last_epoch

        self.T_cur = last_epoch
        self.T_i = T_0
        self.base_lr = optimizer.param_groups[0]['lr']

    def step(self, epoch=None):
        if epoch is None:
            epoch = self.last_epoch + 1

        self.last_epoch = epoch
        self.T_cur = self.T_cur + 1

        if self.T_cur >= self.T_i:
            self.T_cur = 0
            self.T_i = self.T_i * self.T_mult

        # Cosine annealing
        lr = self.eta_min + (self.base_lr - self.eta_min) * \
             (1 + np.cos(np.pi * self.T_cur / self.T_i)) / 2

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

        return lr

    def get_last_lr(self):
        return [group['lr'] for group in self.optimizer.param_groups]


# ==========================================
# âœ… æ”¹è¿›4: æ—©åœæœºåˆ¶
# ==========================================
class EarlyStopping:
    """
    æ—©åœæœºåˆ¶

    åŸç†ï¼š
    ç›‘æ§éªŒè¯æŒ‡æ ‡ï¼Œå¦‚æœè¿ç»­Nä¸ªepochæ²¡æœ‰æ”¹å–„åˆ™åœæ­¢è®­ç»ƒ

    å‚æ•°ï¼š
        patience: å®¹å¿çš„epochæ•°
        min_delta: æœ€å°æ”¹å–„å¹…åº¦
        mode: 'min'ï¼ˆè¶Šå°è¶Šå¥½ï¼‰æˆ–'max'ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
    """
    def __init__(self, patience=30, min_delta=0.001, mode='min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = 0

    def __call__(self, score, epoch):
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
            return False

        if self.mode == 'min':
            improved = score < (self.best_score - self.min_delta)
        else:
            improved = score > (self.best_score + self.min_delta)

        if improved:
            self.best_score = score
            self.best_epoch = epoch
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

        return self.early_stop


# ==========================================
# âœ… æ”¹è¿›5: EMAæ¨¡å‹
# ==========================================
class EMAModel:
    """
    æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¨¡å‹

    åŸç†ï¼š
    ç»´æŠ¤å‚æ•°çš„ç§»åŠ¨å¹³å‡ï¼š
        Î¸_ema = decay * Î¸_ema + (1 - decay) * Î¸

    ä¼˜åŠ¿ï¼š
    - å¹³æ»‘å‚æ•°æ›´æ–°
    - æå‡æ³›åŒ–æ€§èƒ½
    - æ¨ç†æ—¶ç”¨EMAæ¨¡å‹

    å…¸å‹decayï¼š
        0.999: æ ‡å‡†ï¼ˆçº¦1000æ­¥çš„å¹³å‡ï¼‰
        0.9999: é•¿æœŸå¹³å‡ï¼ˆçº¦10000æ­¥ï¼‰
    """
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}

        # åˆå§‹åŒ–shadowå‚æ•°
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        """æ›´æ–°EMAå‚æ•°"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + \
                             self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        """åº”ç”¨EMAå‚æ•°ï¼ˆæ¨ç†å‰è°ƒç”¨ï¼‰"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]

    def restore(self):
        """æ¢å¤åŸå§‹å‚æ•°ï¼ˆæ¨ç†åè°ƒç”¨ï¼‰"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}


# ==========================================
# âœ… æ”¹è¿›6: æŸå¤±è®¡ç®—å™¨ï¼ˆé›†æˆæ‰€æœ‰æŸå¤±ï¼‰
# ==========================================
class CompositeLoss(nn.Module):
    """
    å¤åˆæŸå¤±å‡½æ•°

    æ•´åˆï¼š
    1. åŸºç¡€MSE/åŠ æƒMSE
    2. å¤šå°ºåº¦æŸå¤±
    3. æ¢¯åº¦æŸå¤±
    4. æ„ŸçŸ¥æŸå¤±
    """
    def __init__(self, config, feature_extractor=None):
        super().__init__()
        self.config = config

        # åŠ æƒMSE
        self.use_weighted_mse = config['model'].get('use_weighted_mse', True)
        if self.use_weighted_mse:
            self.low_threshold = config['model'].get('low_traffic_threshold', 0.15)
            self.mid_threshold = config['model'].get('mid_traffic_threshold', 0.5)
            self.low_weight = config['model'].get('low_traffic_weight', 4.0)
            self.mid_weight = config['model'].get('mid_traffic_weight', 2.5)
            self.high_weight = config['model'].get('high_traffic_weight', 2.0)

        # å¤šå°ºåº¦æŸå¤±
        self.use_multiscale = config['model'].get('use_multiscale_loss', True)
        if self.use_multiscale:
            self.multiscale_scales = config['model'].get('multiscale_scales', [1, 2])
            self.multiscale_weights = config['model'].get('multiscale_weights', [1.0, 0.3])

        # æ¢¯åº¦æŸå¤±
        self.use_gradient = config['model'].get('use_gradient_loss', True)
        self.gradient_weight = config['model'].get('gradient_loss_weight', 0.15)

        # æ„ŸçŸ¥æŸå¤±
        self.use_perceptual = config['model'].get('use_perceptual_loss', True)
        self.perceptual_weight = config['model'].get('perceptual_loss_weight', 0.1)

        if self.use_perceptual and feature_extractor is not None:
            perceptual_layers = config['model'].get('perceptual_feature_layers', [2, 3])
            self.perceptual_loss = PerceptualLoss(
                feature_extractor,
                layers=perceptual_layers
            )
        else:
            self.perceptual_loss = None

    def weighted_mse_loss(self, pred, target):
        """åŠ æƒMSEæŸå¤±"""
        mse = (pred - target) ** 2

        # æ ¹æ®æµé‡å¤§å°åˆ†é…æƒé‡
        weights = torch.ones_like(target)
        weights[target < self.low_threshold] = self.low_weight
        weights[(target >= self.low_threshold) & (target < self.mid_threshold)] = self.mid_weight
        weights[target >= self.mid_threshold] = self.high_weight

        return (mse * weights).mean()

    def multiscale_loss(self, pred, target):
        """å¤šå°ºåº¦æŸå¤±"""
        loss = 0.0

        for scale, weight in zip(self.multiscale_scales, self.multiscale_weights):
            if scale == 1:
                loss += weight * F.mse_loss(pred, target)
            else:
                # ä¸‹é‡‡æ ·
                pred_down = F.avg_pool2d(pred, kernel_size=scale, stride=scale)
                target_down = F.avg_pool2d(target, kernel_size=scale, stride=scale)
                loss += weight * F.mse_loss(pred_down, target_down)

        return loss

    def gradient_loss(self, pred, target):
        """æ¢¯åº¦æŸå¤±ï¼ˆä¿æŒè¾¹ç¼˜æ¸…æ™°ï¼‰"""

        # âœ… æ·»åŠ è¿™æ®µç»´åº¦æ£€æŸ¥ä»£ç 
        if pred.dim() == 3:
            pred = pred.unsqueeze(1)
        if target.dim() == 3:
            target = target.unsqueeze(1)

        if pred.size(1) != 1:
            pred = pred.mean(dim=1, keepdim=True)
        if target.size(1) != 1:
            target = target.mean(dim=1, keepdim=True)

        # Sobelç®—å­
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],
                               dtype=pred.dtype, device=pred.device).view(1, 1, 3, 3)
        sobel_y = sobel_x.transpose(2, 3)

        # è®¡ç®—æ¢¯åº¦
        pred_grad_x = F.conv2d(pred, sobel_x, padding=1)
        pred_grad_y = F.conv2d(pred, sobel_y, padding=1)

        target_grad_x = F.conv2d(target, sobel_x, padding=1)
        target_grad_y = F.conv2d(target, sobel_y, padding=1)

        # æ¢¯åº¦æŸå¤±
        loss = F.mse_loss(pred_grad_x, target_grad_x) + \
               F.mse_loss(pred_grad_y, target_grad_y)

        return loss

    def forward(self, pred, target):
        """
        Args:
            pred: [B, 1, H, W] - é¢„æµ‹
            target: [B, 1, H, W] - çœŸå®å€¼
        Returns:
            loss: æ ‡é‡
            loss_dict: å„é¡¹æŸå¤±çš„å­—å…¸ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        """

        # âœ… æ·»åŠ è¿™æ®µç»´åº¦æ£€æŸ¥ä»£ç 
        if pred.dim() == 3:
            pred = pred.unsqueeze(1)
        if target.dim() == 3:
            target = target.unsqueeze(1)

        if pred.size(1) != 1:
            pred = pred.mean(dim=1, keepdim=True)
        if target.size(1) != 1:
            target = target.mean(dim=1, keepdim=True)

        assert pred.shape == target.shape, \
            f"Shape mismatch: pred {pred.shape} vs target {target.shape}"

        loss_dict = {}
        total_loss = 0.0

        # è®¡ç®—MSE/MAE/MAPEç”¨äºç›‘æ§
        pure_mse = F.mse_loss(pred, target)
        loss_dict['pure_mse'] = pure_mse.item()

        mae = F.l1_loss(pred, target)
        loss_dict['mae'] = mae.item()

        mape = torch.mean(torch.abs((pred - target) / target)) * 100
        loss_dict['mape'] = mape.item()

        # 1. åŸºç¡€MSE/åŠ æƒMSE
        if self.use_weighted_mse:
            mse_loss = self.weighted_mse_loss(pred, target)
            loss_dict['weighted_mse'] = mse_loss.item()
        else:
            mse_loss = F.mse_loss(pred, target)
            loss_dict['mse'] = mse_loss.item()

        total_loss += mse_loss

        # 2. å¤šå°ºåº¦æŸå¤±
        if self.use_multiscale:
            ms_loss = self.multiscale_loss(pred, target)
            loss_dict['multiscale'] = ms_loss.item()
            total_loss += ms_loss

        # 3. æ¢¯åº¦æŸå¤±
        if self.use_gradient:
            grad_loss = self.gradient_loss(pred, target)
            loss_dict['gradient'] = grad_loss.item()
            total_loss += self.gradient_weight * grad_loss

        # 4. æ„ŸçŸ¥æŸå¤±
        if self.use_perceptual and self.perceptual_loss is not None:
            perc_loss = self.perceptual_loss(pred, target)
            loss_dict['perceptual'] = perc_loss.item()
            total_loss += self.perceptual_weight * perc_loss

        loss_dict['total'] = total_loss.item()

        return total_loss, loss_dict


# ==========================================
# âœ… æ”¹è¿›7: å®Œæ•´çš„è®­ç»ƒå™¨
# ==========================================
class ImprovedTrainer:
    """
    æ”¹è¿›çš„è®­ç»ƒå™¨ - æ•´åˆæ‰€æœ‰æ”¹è¿›
    """
    def __init__(
        self,
        model,
        train_loader,
        val_loader,
        config,
        device='cuda',
        feature_extractor=None
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device

        # ä¼˜åŒ–å™¨
        self.optimizer = self._build_optimizer()

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._build_scheduler()

        # æ··åˆç²¾åº¦
        self.use_amp = config['training'].get('use_amp', True)
        self.scaler = GradScaler() if self.use_amp else None

        # æ¢¯åº¦ç´¯ç§¯
        self.grad_accum_steps = config['training'].get('gradient_accumulation_steps', 1)

        # MixUp
        mixup_config = config['data'].get('augmentation', {})
        self.mixup = MixUpAugmentation(
            alpha=mixup_config.get('mixup_alpha', 0.2),
            prob=mixup_config.get('mixup_prob', 0.5)
        )

        # æŸå¤±å‡½æ•°
        self.criterion = CompositeLoss(config, feature_extractor)

        # EMA
        self.use_ema = config['training'].get('use_ema', True)
        if self.use_ema:
            ema_decay = config['training'].get('ema_decay', 0.999)
            self.ema = EMAModel(self.model, decay=ema_decay)
        else:
            self.ema = None

        # æ—©åœ
        early_stop_config = config.get('early_stopping', {})
        if early_stop_config.get('enabled', False):
            self.early_stopping = EarlyStopping(
                patience=early_stop_config.get('patience', 30),
                min_delta=early_stop_config.get('min_delta', 0.001),
                mode=early_stop_config.get('mode', 'min')
            )
        else:
            self.early_stopping = None

        # è·¯å¾„
        self.output_dir = Path(config['paths']['output_dir'])
        self.checkpoint_dir = Path(config['paths']['checkpoint_dir'])
        self.log_dir = Path(config['paths']['log_dir'])

        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # æ—¥å¿—
        self.train_losses = []
        self.val_losses = []
        self.learning_rates = []

        # æœ€ä½³æŒ‡æ ‡
        self.best_val_loss = float('inf')
        self.best_epoch = 0

    def _build_optimizer(self):
        """æ„å»ºä¼˜åŒ–å™¨"""
        opt_config = self.config['training']
        opt_type = opt_config.get('optimizer', 'adamw').lower()

        params = self.model.parameters()
        lr = opt_config['learning_rate']
        weight_decay = opt_config.get('weight_decay', 0.05)

        if opt_type == 'adamw':
            optimizer = torch.optim.AdamW(
                params,
                lr=lr,
                betas=(opt_config.get('beta1', 0.9), opt_config.get('beta2', 0.999)),
                weight_decay=weight_decay
            )
        elif opt_type == 'adam':
            optimizer = torch.optim.Adam(
                params,
                lr=lr,
                betas=(opt_config.get('beta1', 0.9), opt_config.get('beta2', 0.999)),
                weight_decay=weight_decay
            )
        else:
            raise ValueError(f"Unknown optimizer: {opt_type}")

        return optimizer

    def _build_scheduler(self):
        """æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        sch_config = self.config['training']
        sch_type = sch_config.get('lr_scheduler', 'cosine_with_restarts').lower()

        if sch_type == 'cosine_with_restarts':
            restart_period = sch_config.get('restart_period', 100)
            min_lr = sch_config.get('min_lr', 1e-7)
            scheduler = CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=restart_period,
                eta_min=min_lr
            )
        elif sch_type == 'cosine':
            num_epochs = sch_config['num_epochs']
            min_lr = sch_config.get('min_lr', 1e-7)
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=num_epochs,
                eta_min=min_lr
            )
        elif sch_type == 'step':
            step_size = sch_config.get('lr_step_size', 50)
            gamma = sch_config.get('lr_gamma', 0.1)
            scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=step_size,
                gamma=gamma
            )
        else:
            scheduler = None

        return scheduler

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        self.mixup.train()

        epoch_loss = 0.0
        epoch_loss_dict = {}

        num_batches = len(self.train_loader)

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")

        for batch_idx, batch in enumerate(pbar):
            # æ•°æ®åŠ è½½
            images = batch['history'].to(self.device)  # [B, T, C, H, W]
            target = batch['target'].to(self.device)  # [B, 1, H, W]

            # MixUpå¢å¼º
            images, target, lam = self.mixup(images, target)

            # å‰å‘ä¼ æ’­
            with autocast(enabled=self.use_amp):
                # è¿™é‡Œéœ€è¦å®Œæ•´çš„å‰å‘ä¼ æ’­ï¼ˆåŒ…æ‹¬vision encoder, qwen, condition encoder, unetï¼‰
                # ç®€åŒ–ç‰ˆæœ¬ï¼Œå‡è®¾modelå·²ç»å°è£…å¥½
                output = self.model(images, target)  # è¿”å›é¢„æµ‹çš„å™ªå£°

                # è®¡ç®—æŸå¤±
                loss, loss_dict = self.criterion(output['pred'], target)

                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / self.grad_accum_steps

            # åå‘ä¼ æ’­
            if self.use_amp:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            # æ¢¯åº¦æ›´æ–°ï¼ˆæ¯grad_accum_stepsæ­¥ï¼‰
            if (batch_idx + 1) % self.grad_accum_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if self.config['training'].get('clip_grad_norm'):
                    if self.use_amp:
                        self.scaler.unscale_(self.optimizer)

                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config['training']['clip_grad_norm']
                    )

                # ä¼˜åŒ–å™¨æ­¥è¿›
                if self.use_amp:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()

                self.optimizer.zero_grad()

                # EMAæ›´æ–°
                if self.ema is not None:
                    self.ema.update()

            # ç»Ÿè®¡
            epoch_loss += loss.item() * self.grad_accum_steps

            for key, value in loss_dict.items():
                epoch_loss_dict[key] = epoch_loss_dict.get(key, 0) + value

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': loss.item() * self.grad_accum_steps,
                'mse': loss_dict.get('pure_mse', 0.0),  # æ˜¾ç¤ºçº¯ MSE
                'mae': loss_dict.get('mae', 0.0),  # æ˜¾ç¤ºçº¯ MAE
                'mape': loss_dict.get('mape', 0.0),
                'lr': self.optimizer.param_groups[0]['lr']
            })

        # å¹³å‡æŸå¤±
        epoch_loss /= num_batches
        for key in epoch_loss_dict:
            epoch_loss_dict[key] /= num_batches

        return epoch_loss, epoch_loss_dict

    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()
        self.mixup.eval()

        # ä½¿ç”¨EMAæ¨¡å‹ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.ema is not None:
            self.ema.apply_shadow()

        val_loss = 0.0
        val_loss_dict = {}

        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                images = batch['history'].to(self.device)
                target = batch['target'].to(self.device)

                # å‰å‘ä¼ æ’­
                output = self.model(images, target)

                # è®¡ç®—æŸå¤±
                loss, loss_dict = self.criterion(output['pred'], target)

                val_loss += loss.item()
                for key, value in loss_dict.items():
                    val_loss_dict[key] = val_loss_dict.get(key, 0) + value

        # æ¢å¤åŸå§‹æ¨¡å‹
        if self.ema is not None:
            self.ema.restore()

        # å¹³å‡æŸå¤±
        val_loss /= len(self.val_loader)
        for key in val_loss_dict:
            val_loss_dict[key] /= len(self.val_loader)

        return val_loss, val_loss_dict

    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }

        if self.ema is not None:
            checkpoint['ema_shadow'] = self.ema.shadow

        # ä¿å­˜æœ€æ–°
        path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'
        torch.save(checkpoint, path)

        # ä¿å­˜æœ€ä½³
        if is_best:
            best_path = self.checkpoint_dir / 'best_model.pt'
            torch.save(checkpoint, best_path)
            print(f"âœ… Saved best model at epoch {epoch}")

        # æ¸…ç†æ—§checkpointï¼ˆä¿ç•™æœ€è¿‘5ä¸ªï¼‰
        checkpoints = sorted(self.checkpoint_dir.glob('checkpoint_epoch_*.pt'))
        if len(checkpoints) > 5:
            for old_ckpt in checkpoints[:-5]:
                old_ckpt.unlink()

    def plot_curves(self, epoch):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))

        # Lossæ›²çº¿
        axes[0].plot(self.train_losses, label='Train Loss', linewidth=2)
        axes[0].plot(self.val_losses, label='Val Loss', linewidth=2)
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('Training and Validation Loss')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # å­¦ä¹ ç‡æ›²çº¿
        axes[1].plot(self.learning_rates, linewidth=2, color='orange')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Learning Rate')
        axes[1].set_title('Learning Rate Schedule')
        axes[1].grid(True, alpha=0.3)
        axes[1].set_yscale('log')

        plt.tight_layout()
        plt.savefig(self.log_dir / f'training_curves_epoch_{epoch}.png', dpi=150)
        plt.close()

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        num_epochs = self.config['training']['num_epochs']
        warmup_epochs = self.config['training'].get('warmup_epochs', 0)
        log_interval = self.config['training'].get('log_interval', 50)
        val_interval = self.config['training'].get('val_interval', 5)
        save_interval = self.config['training'].get('save_interval', 25)

        print("="*60)
        print("ğŸš€ Starting Training")
        print("="*60)
        print(f"Total epochs: {num_epochs}")
        print(f"Batch size: {self.config['training']['batch_size']}")
        print(f"Gradient accumulation: {self.grad_accum_steps}")
        print(f"Effective batch size: {self.config['training']['batch_size'] * self.grad_accum_steps}")
        print(f"Device: {self.device}")
        print(f"Mixed precision: {self.use_amp}")
        print(f"EMA: {self.use_ema}")
        print(f"MixUp: alpha={self.mixup.alpha}, prob={self.mixup.prob}")
        print("="*60)

        for epoch in range(1, num_epochs + 1):
            epoch_start_time = time.time()

            # Warmupé˜¶æ®µ
            if epoch <= warmup_epochs:
                lr_scale = epoch / warmup_epochs
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = self.config['training']['learning_rate'] * lr_scale

            # è®­ç»ƒ
            train_loss, train_loss_dict = self.train_epoch(epoch)
            self.train_losses.append(train_loss)

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None and epoch > warmup_epochs:
                self.scheduler.step(epoch)

            current_lr = self.optimizer.param_groups[0]['lr']
            self.learning_rates.append(current_lr)

            # éªŒè¯
            if epoch % val_interval == 0:
                val_loss, val_loss_dict = self.validate(epoch)
                self.val_losses.append(val_loss)

                # æ‰“å°ä¿¡æ¯
                epoch_time = time.time() - epoch_start_time
                print(f"\nEpoch {epoch}/{num_epochs} - Time: {epoch_time:.2f}s")
                print(f"Train Loss: {train_loss:.6f}")
                print(f"Val Loss:   {val_loss:.6f}")
                print(f"LR:         {current_lr:.2e}")

                # æ£€æŸ¥æ˜¯å¦æœ€ä½³
                is_best = val_loss < self.best_val_loss
                if is_best:
                    self.best_val_loss = val_loss
                    self.best_mse = val_loss_dict['pure_mse']
                    self.best_mae = val_loss_dict['mae']
                    self.best_mape = val_loss_dict['mape']
                    self.best_epoch = epoch

                print(f"Best Val Loss: {self.best_val_loss:.6f} (Epoch {self.best_epoch})")
                print(f"Best MSE: {self.best_mse:.6f}")
                print(f"Best MAE: {self.best_mae:.6f}")
                print(f"Best MAPE: {self.best_mape:.6f}")

                # æ—©åœæ£€æŸ¥
                if self.early_stopping is not None:
                    if self.early_stopping(val_loss, epoch):
                        print(f"\nâš ï¸ Early stopping triggered at epoch {epoch}")
                        print(f"Best epoch was {self.best_epoch} with loss {self.best_val_loss:.6f}")
                        break

            # ä¿å­˜checkpoint
            if epoch % save_interval == 0 or epoch == num_epochs:
                self.save_checkpoint(epoch, is_best=(epoch == self.best_epoch))

            # ç»˜åˆ¶æ›²çº¿
            if epoch % (val_interval * 2) == 0:
                self.plot_curves(epoch)

        print("\n" + "="*60)
        print("âœ… Training completed!")
        print(f"Best validation loss: {self.best_val_loss:.6f} at epoch {self.best_epoch}")
        print("="*60)

        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint(num_epochs, is_best=False)
        self.plot_curves(num_epochs)

        # ä¿å­˜è®­ç»ƒå†å²
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'best_val_loss': self.best_val_loss,
            'best_epoch': self.best_epoch
        }

        with open(self.log_dir / 'training_history.json', 'w') as f:
            json.dump(history, f, indent=2)
